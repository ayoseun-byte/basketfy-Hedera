package agent

import (
	"bytes"
	"encoding/json"
	"fmt"
	"basai/config"

	"strings"
	"basai/domain/ai/agent/tools"
	"basai/domain/ai/llms"
	"basai/domain/ai/utilities"
)

// _executeToolAction is a function that takes an action string, an actionInput string, and an actionMeta map[string]interface{}
// as input parameters. It returns a string, an interface{}, a types.NotePad, and a string.
//
// The purpose of this function is to retrieve the appropriate tool function based on the provided action string
// and execute it with the given actionInput and actionMeta. It then returns the result of the tool function
// along with the raw tool response, any note data generated, and the intent ID of the tool.
//
// Here's a breakdown of the function's logic:
//
//  1. It checks if the provided action string exists in the multimodal.Alltools.AllCliveTools map. If not found,
//     it returns a custom formatted error message indicating that the selected tool is wrong, along with a list
//     of available tool names. It also returns nil for the raw tool response, an empty types.NotePad, and an empty string
//     for the intent ID.
//
// 2. If the tool is found, it retrieves the corresponding tool function from the map.
//
//  3. It executes the tool function by calling tool.ToolFunc(actionInput, actionMeta). This function likely performs
//     some operation based on the provided actionInput and actionMeta, and returns the result, raw tool response,
//     and any note data generated.
//
// 4. The function then returns the result, raw tool response, note data, and the intent ID of the tool.
//
// Parameters:
// - action (string): The name or identifier of the tool to be executed.
// - actionInput (string): The input data or prompt to be provided to the tool function.
// - actionMeta (map[string]interface{}): Additional metadata or parameters required by the tool function.
//
// Returns:
// - string: The result of executing the tool function.
// - interface{}: The raw response from the tool function.
// - types.NotePad: Any note data generated by the tool function.
// - string: The intent ID of the selected tool.
func _executeToolAction(tradingTools tools.BasaiTools, toolNames, action, actionInput string, actionMeta map[string]interface{}) (string, any, tools.NotePad, string) {
	// Remove leading or trailing punctuation marks from action
	action = strings.Trim(action, ".,!?;:'")

	// Attempt to find the requested tool in the AllTokraiTools map
	tool, found := tradingTools.AllTokraiTools[action]
	if !found {
		// If the tool is not found, return the error message with the requested tool name and available tool names
		return string(utilities.CustomFormat([]byte(`You tried to use {tool} tool, but it doesn't exist. You must use any of these available tools: {name_of_tools}.`), map[string][]byte{`tool`: []byte(action), `name_of_tools`: []byte(toolNames)})), nil, tools.NotePad{}, ""
	}

	// If the tool is found, call its ToolFunc with the provided input and metadata
	result, rawToolResponse, noteData := tool.ToolFunc(actionInput, tool.Name, actionMeta)
	// Return the result, raw tool response, note data, and the tool's intent ID
	return result, rawToolResponse, noteData, tool.IntentId
}

// startLLMClient initializes and starts a language model client based on the specified provider.
// It uses the provided LLMProvider configuration to determine which client to start and passes
// the necessary API key and parameters to the client's streaming function.
//
// Parameters:
// - LLMProvider ([]string): A slice containing the provider name and model identifier.
// - kwargs (map[string]interface{}): A map of additional parameters for the language model client.
func startLLMClient(LLMProvider []string, kwargs map[string]interface{}) (string, error) {
	// Create a new instance of the language model client interface
	var (
		clm          llms.CallLLmInterface = llms.NewLLMClient()
		chatResponse string
		chatErr      error
	)

	// Check if the LLMProvider configuration is valid
	if len(LLMProvider) < 2 {
		fmt.Print("Invalid LLMProvider configuration")
		return "", fmt.Errorf("%v", "invalid LLMProvider configuration")
	}

	// Set the model in the kwargs map using the second element of LLMProvider
	kwargs["model"] = LLMProvider[1]

	// Determine which language model client to start based on the provider name
	switch LLMProvider[0] {
	case "gemini":
		kwargs["model"] = LLMProvider[1]
		chatResponse, chatErr = clm.GemStreamCompleteClient(config.AppConfig.GeminiAPIKey, kwargs)
	default:
		// Log an error if the provider is unsupported
		fmt.Print("Unsupported LLM provider:", LLMProvider[0])
	}
	return chatResponse, chatErr
}

func RebalancerAgent(agentSynapse Synapse, LLMProvider []string,tokens interface{}, verbose bool) (string, []map[string]interface{}, error) {
	// Initialize a list to store the responses from the tools
	var (
		toolResponseList []map[string]interface{}
		actionReady      bool
		newChatResponse  string
		chatErr          error
		chatResponse     string
		mmAgent          MultiModalAgentInterface = NewMultiModalAgent()
	)
	// Prepare the prompt and retrieve necessary data
	systemPrompt, chatHistory, partnerTools, toolNames, _ := mmAgent.PreparePrompt(agentSynapse, []byte(LLMProvider[0]),tokens)

	// Initialize AI Identity
	agentSynapse.AIIdentity = []byte("\nTokrai: ")

	// Append the user's prompt to the chat history with the role specified as "user"
	chatHistory = append(chatHistory, map[string]string{"role": "user", "content": agentSynapse.UserPrompt})

	newMap := map[string]string{"role": "system", "content": string(systemPrompt)}

	// Prepend the new map
	chatHistory = append([]map[string]string{newMap}, chatHistory...)

	// Log chat history if verbose is enabled
	if verbose {
		chatHistoryJSON, err := json.Marshal(chatHistory)
		if err != nil {
			fmt.Print(fmt.Errorf("error marshaling chat history: %w", err))
		} else {
			utilities.Printer("Chat History JSON: ", string(chatHistoryJSON), "gold")
		}
	}

	// Set up keyword arguments for the language model call
	kwargs := map[string]interface{}{
		"max_tokens":  5000,
		"messages":    chatHistory,
		"temperature": 0.3,
		"stream":      false,
		"stop":        []string{},
	}

	chatResponse, chatErr = startLLMClient(LLMProvider, kwargs)

	if chatErr != nil {
		return "", nil, chatErr
	}
	agentActionTypes, _, neupErr := NeuralParser([]byte(chatResponse), LLMProvider[0], true)

	if neupErr != nil {
		return "", nil, neupErr
	}

	if finish, ok := agentActionTypes.AgentFinish["Output"]; ok {
		utilities.Printer("", string(finish), "green")
		if toolResponseList != nil {
			return string(finish), toolResponseList, nil
		}
		return string(finish), []map[string]interface{}{}, nil
	}

	_, exists := agentActionTypes.AgentAction["Action"]
	if exists {
		actionReady = true
	}

	for actionReady {

		if newChatResponse != "" {
			chatResponse = newChatResponse
		}

		// Parse agent actions
		agentActions, _, parseErr := NeuralParser([]byte(chatResponse), LLMProvider[0], true)
		if parseErr != nil {
			fmt.Print(parseErr)
			return "", nil, parseErr
		}

		if finish, ok := agentActions.AgentFinish["Output"]; ok {
			actionReady = false
			utilities.Printer("", string(finish), "green")
			cleanedFinish := strings.TrimSuffix(strings.TrimPrefix(string(finish), "<answer>"), "</answer>")

			if toolResponseList != nil {
				return cleanedFinish, toolResponseList, nil
			}
			return cleanedFinish, []map[string]interface{}{}, nil
		}

		if action, exists := agentActions.AgentAction["Action"]; exists {

			// Process feedback if available
			if verbose {
				utilities.Printer("\n", string(agentActions.AgentAction["Feedback"]), "blue")
			}

			// Get tool response
			toolResponse, rawToolResponse, notepadData, toolIntentId := _executeToolAction(
				partnerTools,
				toolNames,
				string(action),
				string(agentActions.AgentAction["Input"]),
				agentSynapse.MetaData,
			)

			if verbose {
				utilities.Printer("Observation: ", toolResponse, "purple")
			}
		
			noteService := tools.NewNoteService(notepadData, agentSynapse.UserId, agentSynapse.UserId,0)
			noteService.SaveNote()
			// Append tool response to list
			toolResponseList = append(toolResponseList, map[string]interface{}{toolIntentId: rawToolResponse})

			// Build agent thought process in a single operation to reduce allocations
			var agentThoughtBuf bytes.Buffer
			agentThoughtBuf.Write(agentSynapse.AIIdentity)
			agentThoughtBuf.WriteString("\n" + chatResponse)
			agentThoughtBuf.WriteString("\n\nTool Response:\n")
			agentThoughtBuf.WriteString(toolResponse)
			agentThoughtBuf.WriteString("\n")
			agentThoughtBuf.Write(agentSynapse.AIIdentity)

			// Append the new prompt to the chat history
			chatHistory = append(chatHistory, map[string]string{"role": "user", "content": agentThoughtBuf.String()})

			// Update existing kwargs map for the next language model call
			kwargs["messages"] = chatHistory

			kwargs := map[string]interface{}{
				"max_tokens":  5000,
				"messages":    chatHistory,
				"temperature": 0.3,
				"stream":      false,
				"stop":        []string{},
			}

			chatResponse, chatErr = startLLMClient(LLMProvider, kwargs)

			if chatErr != nil {
				return "", nil, chatErr
			}
		}
	}

	return "", nil, nil
}
